\documentclass{article}

\title{\textbf{Master's Thesis defense}}
\author{Giovanni Demasi}
\date{September 2023}

\begin{document}

\maketitle

\section{Speech}

\noindent
% 01
Hi, my name is Giovanni Demasi and today I am going to discuss my thesis about an FPGA toolchain for Graph Neural Network acceleration using High-Level synthesis, supervised by Professor Fabrizio Ferrandi , and by doctors Serena Curzel and Michele Fiorito.
\\
\\
\noindent
% 02
This presentation is going to firstly introduce which were the objectives of this thesis, followed by a brief summary about background knowledge.
Then, the proposed solution will be introduced, along with experiments that support it.
Finally, a summary of the study and some possible future developments will be discussed.
\\
\\
\noindent
% 03
Graph neural networks have been introduced in response to the growing demand for learning tasks involving graph data, particularly in domains characterized by vast amounts of data, such as social networks and chemistry.
Optimizing and accelerating the capabilities of Graph Neural Networks is necessary due to their increasingly popularity, especially for inference time, which refers to the time the model takes to make predictions.
For this reason, the main goal of this thesis was to develop a comprehensive toolchain that, starting directly from an high-level framework, enables the automatic generation of a graph neural networks FPGA accelerator with minimal effort required.
To do so, part of the expected work I was supposed to do includes identify the toolchain's elements, enhancing them to make them compatible and then effectively accelerate GNN inference.
\\
\\
\noindent
% 04
Graph neural networks are deep learning techniques that operate on graph-structured data to solve prediction tasks that can be classified into three most popular categories: graph classification, node classification, and link predictions.
Graph Neural Networks consist of multiple interconnected layers and typically encompasses three main stages: pre-processing for transforming the input data, iterative updates of feature vectors, and readout to aggregate information from node and edge embeddings to produce a global feature vector of the graph.
\\
\\
\noindent
% 05
As already said, the main contribution of this thesis is represented by the design of a toolchain for Graph Neural Network acceleration on FPGA leveraging High-Level Synthesis.
In particular, the toolchain exploits MLIR and its Intermediate Representation.
MLIR is a novel approach to construct reusable and extensible compiler infrastructure and its intermediate representation is used as input for the synthesizer which leverages Bambu, an High-Level Synthesis tool.
HLS simplifies hardware development by allowing users to provide programs written in common programming languages obtaining their VHDL and Verilog implementation without the need of low-level programming.
The MLIR intermediate representation serves as input for the synthesizer, where, once the frontend optimization is complete, the refined version proceeds to the backend, where the actual GNN accelerator is effectively produced.
\\
\\
\noindent
% 06
The contributions of this thesis start with an analysis of existing tools of GNN implementation.
Then, I have made a list of general rules to make any GNN model compatible with torchscript, ready to be used with the proposed toolchain.
I have also contributed in enhancing Torch-MLIR: I have added a new feature, the support of the constant of Tuple type, now available in the main public repository of Torch-MLIR, and I have also identified different areas of improvement in order to enhance the compatibility between PyTorch Geometric and Torch-MLIR.
The bottlenekcs analysis of the model allowed me to explore the design space, creating a new optimization pipeline for GNN acceleration with fine tuned optimizations.
\\
\\
\noindent
% 07
The entry point of the toolchain is represented by PyTorch, one of the most popular and powerful framework for neural network implementations, and the first step of the flow consists in the implementation of the model in PyTorch.
The main model I have used in this thesis is a Graph Convolutional Network implemented in Pytorch, with two convolutional layers, whose forward function is mainly made of two matrix multiplications, one of which is sparse.
The dataset I have used is the Cora dataset, a multi-class classification dataset which contains different scientific publications, in which the objective is to classify a paper into one of the seven classes correctly according to its category.
In this case, the Cora dataset represents the graph, with information about nodes, edges and features, and its size influences the computational time of both training and inference.
For this reason, in order to keep the synthesis and evaluation times contained, for the experimental phase different subsets of the original Cora dataset have been prepared and used.
\\
\\
\noindent
% 08
To being able to apply fine-tuned synthesis optimizations in the next phases of the toolchain, the first thing I have done is an analysis about model performance to identify its computational bottlenecks.
The results showed that more than half of the computation was used by matrix multiplication.
As previously said, the model uses both dense and sparse matrix multiplication, but at the actual state Torch-MLIR does not support sparse tensors, so an analysis has been performed to understand the performance differences between dense and sparse matrix multiplication in PyTorch.
The results of this analysis showed that sparse matrix multiplication performance is faster than dense only in case of big matrix sizes with density lower or equal than 0.001\%.
However, even if the following experiments do not take advantage of sparse matrix multiplications,
the usage of custom optimization has enabled the acceleration of inference, balancing the absence of sparse operations.
\\
\\
\noindent
% 09
Torch-MLIR is a crucial middle step that enables the generation of the MLIR representation.
After I have identified the dataset and the PyTorch GNN model to use, the next thing I have done is to make the model compatible with torch script, an intermediate representation used to generate serializable and optimizable models directly from PyTorch code.
Once I have made the model compatible with Torch Script, generalizing the changes done in order to make them usable with different models, and after having trained the GNN model in PyTorch, I have used torch-MLIR to obtain the MLIR representation of the model.
\\
\\
\noindent
% 10
The synthesizer represents the final step of the toolchain, which optimizes and synthesizes the MLIR representation, targeting FPGA. This step includes SODA-OPT and PandA-Bambu.
In particular, SODA-OPT receives as input the MLIR representation of the model with the code to accelerate that I have previously outlined. This step is primarily responsible for applying optimizations that can be exploited in the next step. In particular, a subset of MLIR passes can be used to do so.
The output of SODA-OPT serves as input to PandA-Bambu, which represents the last phase of the synthesis.
Bambu removes the need of manual low-level programming, it significantly reduces design time and effort, making the final implementation’s functionality independent of hardware design knowledge.
Bambu, after having applied some optional low-level optimizations, produces the accelerator as final output.
\\
\\
\noindent
% 11
Being matrix multiplication the most expensive computation of the model, firstly I wanted to understand how to optimize it, in order to then apply these optimizations to the whole model.
The toolchain offers the possibility to apply different optimizations during synthesis, but before doing so, I thought it was necessary to understand the performance difference between PyTorch matrix multiplication recorded on an Intel Core i9 with 8 cores and the baseline accelerator targeting FPGA.
To obtain the baseline matrix multiplication accelerator, I have outlined only the matrix multiplication code of the generated MLIR representation through SODA annotations, which I have then synthesized with Bambu.
By default, Bambu uses two memory channels storing all memory objects in on-chip block RAMs.
The baseline accelerator is much faster than PyTorch when matrices are relatively small but it becomes slower for bigger matrix sizes.
The reasons behind this behavior may lie in the fact that the generated accelerator assumes that all data is available in BRAM, and also in the fact that PyTorch times have been measured using all available threads on the machine, exploiting more parallelism with respect to the accelerator.
\\
\\
\noindent
% 12
In order to make the accelerator able to exploits more parallelism, I have mainly applied two optimizations.
Regarding high-level optimizations of SODA-OPT, I have achieved the biggest improvement in terms of performance by applying the loop unrolling technique, which involves expanding completely or partially the loop, according to a parameter called unroll factor, which can be set to decide the number of loop iterations that are unrolled.
The optimization I have used in Bambu consisted in increasing the number of memory channels, using an external memory, which allows to better exploit the high level of parallelism that can be achieved in combination with the loop unrolling technique.
However, using the external memory increased the number of execution cycles and I have analysed this behaviour.
\\
\\
\noindent
% 13
The study I have conducted revealed that there is a point, represented by a specific unroll factor, after which using an external memory with 32 channels becomes beneficial.
As it is visible by the two following plots, which analysed the number of cycles needed using different unroll factors, when the parallelization is not high, using two channels storing all memory objects in BRAMs is the best option. Using 16 channels seems to be similar to using 32 channels, but when parallelization is high the latter option uses fewer cycles.
As expected, the findings indicate that opting for two channels with external memory is the least favorable choice. This is primarily because employing external memory becomes advantageous when a large volume of data can be simultaneously loaded and stored to offset the additional cycles required by external memory.
\\
\\
\noindent
% 14
Having obtained promising results on matrix multiplication, I have applied the analysed optimizations to the Graph Convolutional Network.
This plot shows the result of a comparative analysis I have performed between the PyTorch CPU time and the optimized FPGA accelerator time to perform GCN inference using different sizes of the dataset.
The results of this evaluation are incredibly encouraging, showing significant improvement obtained by the optimized accelerator using two channels with on-chip BRAMs.
The inference time of the accelerator is significantly lower with respect to the one measured on CPU with PyTorch, and this advantage is maintained also with bigger dataset sizes.
This improvement in performance did not affect the model accuracy, which is the same on both PyTorch and FPGA implementations.
Additionally, as expected, being matrix multiplication the most time-consuming operation of the model, by applying the loop unrolling technique the speedup of the optimized accelerator compared to the baseline accelerator increases as the size of the dataset increases.
\\
\\
\noindent
% 15
Accelerating Graph Neural Networks has become a subject of interest within the research community, exploring mainly FPGA accelerators.
An interesting comparison involves FlowGNN, which stands out among the state-of-the-art technologies as the only solution employing the HLS technique, which I have also used in this thesis.
Almost all the state of the art accelerators only support a limited set of models. Instead, the toolchain proposed in this thesis offers support for a diverse range of GNN models, allowing to generate the accelerator directly from the PyTorch implementation, eliminating the need for low-level programming.
This sets it apart from FlowGNN, which offers pre-built models in C++.
These models can be slightly changed according to different features, however to do this a minimum of experience with C++ is required, and if the changes are substantial it could be a blocking factor.
On the contrary, the proposed toolchain, only requires knowledge in PyTorch, one of the most used high-level framework for GNN implementations.
One advantage of the proposed design flow is that it offers the flexibility of using customized settings tailored to various applications.
Most state of the art solutions are not customizable and not adaptable to new functionalities.
In particular, FlowGNN is limited to just four configurable parallelization parameters and focuses on delivering model-specific components, which limit the customization of the accelerator even further.
In contrast, the proposed toolchain offers many optimization passes from both SODA-OPT and PandA-Bambu, allowing users to finely customize and optimize the accelerator’s capabilities.
\\
\\
\noindent
% 16
In conclusion, I have faced the challenge of accelerating Graph Neural Network inference.
In particular, I have listed general rules to make GNN models compatible with torch script.
I have contributed in enhancing Torch-MLIR with the addition of a new feature, plus the identification of different areas of improvement to enhance the compatibility between PyTorch Geometric and Torch-MLIR.
Before this research, no examples were available on how to use Torch-MLIR with Graph Neural Networks, and the compatibility of PyTorch Geometric and Torch-MLIR was still an unexplored area.
Another improvement that can be applied to the presented toolchain is the support of sparse tensors.
At the actual state, Torch-MLIR is not capable of lowering PyTorch sparse tensors, even if MLIR already supports them. A possible solution is represented by PyTaco, an end-to-end use case for the sparse tensor compiler, able to create MLIR operations annotated with sparse tensor information.
\\
\\
\noindent
% 17
This was a summary of the entire work done during my thesis, thanks for your attention.

\section{Questions and answers}

\textbf{Why did not you compare your solution also with GPU?} \\
\noindent
Answer...
\\
\\
\noindent
\textbf{Why did not you use the entire Cora dataset?} \\
\noindent
Because the Cora dataset represents the graph, with information about nodes, edges and features, and its size influences the computational time of both training and inference.
In order to keep the synthesis and evaluation times contained, for the experimental phase different subsets of the original Cora dataset have been prepared and used.
\\
\\
\noindent
\textbf{Why did not you use pipelined loop with external memory?} \\
\noindent
Pipelining would probably contribute in achieving even more better results, but since the explored optimizations allowed me to already obtain promising result and to accelerate GNN inference, I have not explored it in this thesis, but it would surely be a nice future developments.
\\
\\
\noindent
\textbf{How does using more channels with external memory is related to real implementations?} \\
\noindent
Answer...
\\
\\
\noindent
\textbf{Why you do not have compared your accelerator performance with FlowGNN?} \\
\noindent
FlowGNN proposes some C++ implementations of the model and the proposed GCN is a different implementation compared to the one used in this thesis. I should have implemented our model in C++ and making it works with FlowGNN, but it would not have been a meaningful comparison.
\\
\\
\noindent

\end{document}
