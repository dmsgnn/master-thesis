% MSc Thesis of Giovanni Demasi - 987062
% Politecnico di Milano (PoliMi) - School of Industrial and Information Engineering

\documentclass{Configuration_Files/PoliMi3i_thesis}

%------------------------------------------------------------------------------
%	REQUIRED PACKAGES AND  CONFIGURATIONS
%------------------------------------------------------------------------------

% CONFIGURATIONS
\usepackage{parskip} % For paragraph layout
\usepackage{setspace} % For using single or double spacing
\usepackage{emptypage} % To insert empty pages
\usepackage{multicol} % To write in multiple columns (executive summary)
\setlength\columnsep{15pt} % Column separation in executive summary
\setlength\parindent{0pt} % Indentation
\raggedbottom

% PACKAGES FOR TITLES
\usepackage{titlesec}
% \titlespacing{\section}{left spacing}{before spacing}{after spacing}
\titlespacing{\section}{0pt}{3.3ex}{2ex}
\titlespacing{\subsection}{0pt}{3.3ex}{1.65ex}
\titlespacing{\subsubsection}{0pt}{3.3ex}{1ex}
\usepackage{color}

% PACKAGES FOR LANGUAGE AND FONT
\usepackage[english]{babel} % The document is in English  
\usepackage[utf8]{inputenc} % UTF8 encoding
\usepackage[T1]{fontenc} % Font encoding
\usepackage[11pt]{moresize} % Big fonts

% PACKAGES FOR IMAGES
\usepackage{graphicx}
\usepackage{transparent} % Enables transparent images
\usepackage{eso-pic} % For the background picture on the title page
\usepackage{subfig} % Numbered and caption subfigures using \subfloat.
\usepackage{tikz} % A package for high-quality hand-made figures.
\usetikzlibrary{}
\graphicspath{{./Images/}} % Directory of the images
\usepackage{caption} % Coloured captions
\usepackage{xcolor} % Coloured captions
\usepackage{amsthm,thmtools,xcolor} % Coloured "Theorem"
\usepackage{float}

% STANDARD MATH PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[overload]{empheq} % For braced-style systems of equations.
\usepackage{fix-cm} % To override original LaTeX restrictions on sizes

% PACKAGES FOR TABLES
\usepackage{tabularx}
\usepackage{longtable} % Tables that can span several pages
\usepackage{colortbl}

% PACKAGES FOR ALGORITHMS (PSEUDO-CODE)
\usepackage{algorithm}
\usepackage{algorithmic}

% PACKAGES FOR REFERENCES & BIBLIOGRAPHY
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref} % Adds clickable links at references
\usepackage{cleveref}
\usepackage[square, numbers, sort&compress]{natbib} % Square brackets, citing references with numbers, citations sorted by appearance in the text and compressed
\bibliographystyle{abbrvnat} % I may use a different style adapted to your field

% OTHER PACKAGES
\usepackage{pdfpages} % To include a pdf file
\usepackage{afterpage}
\usepackage{lipsum} % DUMMY PACKAGE
\usepackage{fancyhdr} % For the headers
\fancyhf{}

% Input of configuration file.
\input{Configuration_Files/config}

%----------------------------------------------------------------------------
%	NEW COMMANDS DEFINED
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	ADDITIONAL PACKAGES
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	ADDITIONAL DEFINITIONS AND COMMANDS
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	DOCUMENT
%----------------------------------------------------------------------------

\begin{document}
    \nocite{*}

    \fancypagestyle{plain}{%
        \fancyhf{} % Clear all header and footer fields
        \fancyhead[RO,RE]{\thepage} %RO=right odd, RE=right even
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

%----------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------

    \pagestyle{empty} % No page numbers
    \frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the preamble pages

    \puttitle{
        title=Graph Neural Network \\ Acceleration with SODA \\ Framework, % Title of the thesis
        name=Giovanni Demasi, % Author Name and Surname
        course=Computer Science and Engineering, % Study Programme
        ID  = 987062,  % Student ID number (numero di matricola)
        advisor= Prof. Fabrizio Ferrandi, % Supervisor name
        coadvisor={Serena Curzel}, % Co-Supervisor name
        academicyear={2022-23},  % Academic Year
    } % These info will be put into your Title page

%----------------------------------------------------------------------------
%	PREAMBLE PAGES: ABSTRACT (inglese e italiano), EXECUTIVE SUMMARY
%----------------------------------------------------------------------------
    \startpreamble
    \setcounter{page}{1} % Set page counter to 1

% ABSTRACT IN ENGLISH
    \chapter*{Abstract}
    Here goes the Abstract in English of your thesis followed by a list of keywords.
    The Abstract is a concise summary of the content of the thesis (single page of text)
    and a guide to the most important contributions included in your thesis.
    The Abstract is the very last thing you write.
    It should be a self-contained text and should be clear to someone who hasn't (yet) read the whole manuscript.
    The Abstract should contain the answers to the main scientific questions that have been addressed in your thesis.
    It needs to summarize the adopted motivations and the adopted methodological approach as well as the findings of your work and their relevance and impact.
    The Abstract is the part appearing in the record of your thesis inside POLITesi,
    the Digital Archive of PhD and Master Theses (Laurea Magistrale) of Politecnico di Milano.
    The Abstract will be followed by a list of four to six keywords.
    Keywords are a tool to help indexers and search engines to find relevant documents.
    To be relevant and effective, keywords must be chosen carefully.
    They should represent the content of your work and be specific to your field or sub-field.
    Keywords may be a single word or two to four words.
    \\
    \\
    \textbf{Keywords:} here, the keywords, of your thesis % Keywords

% ABSTRACT IN ITALIAN
    \chapter*{Abstract in Lingua Italiana}
    Qui va l'Abstract in lingua italiana della tesi seguito dalla lista di parole chiave.
    \\
    \\
    \textbf{Parole chiave:} qui, vanno, le parole chiave, della tesi % Keywords (italian)

%----------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES/SYMBOLS
%----------------------------------------------------------------------------

% TABLE OF CONTENTS
    \thispagestyle{empty}
    \tableofcontents % Table of contents
    \thispagestyle{empty}
    \cleardoublepage

%-------------------------------------------------------------------------
%	THESIS MAIN TEXT
%-------------------------------------------------------------------------
% In the main text of this thesis it is possible to write the chapters in two different ways:
%
%(1) It is possible to write:
%    \chapter{Title of the chapter}
%    *body of the chapter*
%
%(2) It is also possible to write the chapter in a separated .tex file and then include it in the main file with the following command:
%    \chapter{Title of the chapter}
%    \input{chapter_file.tex}
%
% Especially for long thesis, the second option is the recommended one.

    \addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
    \mainmatter % Begin numeric (1,2,3...) page numbering

% ##########################################################################
% CHAPTER ONE - INTRODUCTION
% ##########################################################################

    \chapter{Introduction}
    \label{ch:chapter_one}%

    In recent years, deep learning has brought about a revolutionary transformation in various machine learning tasks,
    spanning from image classification and video processing to speech recognition and natural language understanding.
    Traditionally, these tasks have predominantly operated within the Euclidean space, where data is typically
    represented.
    Nevertheless, a growing number of applications now generate data from non-Euclidean domains,
    presenting it in the form of complex graphs with intricate relationships and interdependencies among objects.
    The inherent complexity of graph data has posed considerable challenges for existing machine learning algorithms.
    Consequently, there has been a surge of studies focusing on extending deep learning techniques to accommodate
    and leverage graph data.

    Graph neural networks (GNNs) have been introduced in response to the growing demand for learning tasks involving
    graph data, which encompasses extensive relational information among its elements.
    These neural models effectively capture the interdependence among graph nodes by employing message passing mechanisms.

    As Graph Neural Networks are increasingly employed, particularly in domains characterized by vast amounts of data,
    such as social networks and chemistry, a need arises to optimize and accelerate their capabilities.
    Inference in GNNs refers to the time the model takes to make predictions after training.
    The duration of the inference process determines the speed at which queries are answered, and researchers strive to minimize this time span.

    In applications of deep learning that prioritize low latency, FPGAs outperform other computing devices, such as CPUs and GPUs,
    by providing superior performance.
    FPGAs offer the advantage of being fine-tuned to strike the optimal balance between power efficiency and meeting performance requirements.

    Due to this reason, researchers have been actively pursuing the development of new FPGA accelerators for Graph Neural Networks (GNNs) in recent times.

    The conventional approach to hardware design involves a combination of manual coding and automated processing.
    However, this method demands significant effort and relies heavily on the expertise of the designers, leading to varying quality of results.

    To address these challenges, the objective of this thesis research study is to develop a comprehensive toolchain that, starting from PyTorch~\cite{DBLP:journals/corr/abs-1912-01703},
    a cutting-edge high-level programming framework for creating neural network algorithms based on the Python programming language, enables the
    automatic generation of a Graph Neural Networks (GNNs) FPGA accelerator with minimal effort required.

    The suggested toolchain represents an enhancement of the SODA toolchain~\cite{9786533}.
    It operates by transforming the PyTorch model, provided as input, into a multi-level intermediate representation
    (MLIR)~\cite{9370308} utilizing Torch-MLIR~\cite{torch_mlir}, an MLIR based compiler toolkit for PyTorch programs.
    This MLIR representation is then passed to the SODA framework to conduct hardware/software partitioning of the algorithm
    specifications and architecture-independent optimizations.
    Following this, the framework generates a low-level IR (LLVM IR) specifically tailored for the hardware generation engine,
    PandA-Bambu~\cite{9586110}.

    In pursuit of the thesis goal, various optimizations were adopted throughout the process.
    Specifically, efforts were made to optimize specific computations in Graph Neural Networks during the experimental phase.
    As these networks often deal with massive graph sizes, the computation time and memory requirements are substantial.
    Consequently, a significant portion of the research focuses on optimizing the computation phase of Graph Neural Networks using
    tailored SODA optimizations, particularly matrix multiplication.

    Furthermore, limitations and challenges have been encountered along the way.
    Another objective of this thesis is to analyze these limitations, ensuring they are clearly understood thoroughly.
    This analysis aims to provide valuable insights for future research endeavors, enabling the development of solutions
    to overcome these limitations and further enhance the proposed toolchain.

    While the intended purpose of the toolchain is to be general, the experimental phase primarily focused on two specific
    types of Graph Neural Networks: Graph Isomorphism Networks (GIN)~\cite{xu2019powerful} and Graph Convolutional Networks (GCN)~\cite{DBLP:journals/corr/KipfW16}.
    These models were sourced from reliable GitHub implementations and were modified as necessary.

    The GCN model~\cite{pygcn}, designed for node classification task and written in pure PyTorch, held particular importance for the
    experimental phase as it served as the basis for the resulting accelerator.
    On the other hand, the GIN model~\cite{ogb_gnn_models}, designed for graph classification task and written in PyTorch Geometric~\cite{DBLP:journals/corr/abs-1903-02428},
    a library built upon PyTorch for easier development and training of Graph Neural Networks, did not progress through
    the final step of the proposed toolchain.
    This was due to some incompatibilities between PyTorch Geometric and Torch-MLIR, which are integral parts of this thesis research.
    
    \section{Contributions}
    \label{sec:contributions}%


    \section{Thesis structure}
    \label{sec:thesis_structure}%

    Chapter~\ref{ch:chapter_one} introduces the context of the thesis, its objective, and its goals, including a general overview of the research's focus, contributions, and outcome.
    Chapter~\ref{ch:chapter_two} presents the background needed to understand the thesis's content deeply.
    In particular, it contains a summary of Graph Neural Networks, how they work, an explanation of the GNN types used in the experimental phase, and the type of tasks that they can perform, including some of their applications.
    Additionally, it presents the SODA framework, an important part of this thesis's proposed toolchain.
    Chapter~\ref{ch:chapter_three} instead contains an overview of the related works.
    Other Graph Neural Network acceleration frameworks will be analyzed, underlying their differences compared to the research study done for this thesis and some limitations.
    Chapter~\ref{ch:chapter_four} formulates the problem statement, summarizes the open issues of the research objective, and explains how the thesis goals can be helpful and their expected impact.
    Chapter~\ref{ch:chapter_five} is the core chapter of the thesis, it clearly explains how the problem has been faced and what technologies have been used.
    It contains a detailed description of the proposed toolchain and its working method.
    Chapter~\ref{ch:chapter_six} lists all the performed experiments, gives the necessary information to reproduce them and contains their outcomes and the issues and limitations encountered.
    Finally, Chapter~\ref{ch:conclusions} presents overall considerations of the study, both with the main achievements obtained and the most notable obstacles faced.
    Along with this, potential improvements for future studies are considered.



% ##########################################################################
% CHAPTER TWO - BACKGROUND
% ##########################################################################


    \chapter{Background}
    \label{ch:chapter_two}%
    % The \label{...}% enables to remove the small indentation that is generated, always leave the % symbol.

    Chapter containing the background needed to understand the problem and its solution, mainly a summary
    of Graph Neural Networks and SODA toolchain.

    \section{Graphs}
    \label{sh:graphs}%

    \textit{Graphs} are a data structure representing a collection of objects, known as vertices or nodes, and a set of edges~\cite{DBLP:journals/corr/abs-1812-08434}.
    In a graph, the edges can be either directed or undirected, and they typically connect two vertices, which may or may not be distinct.
    The vertices represent entities or elements, and the edges represent their relationships or connections.

    Graphs serve as a versatile tool for describing diverse forms of data.
    Molecules, the fundamental units of matter, are composed of atoms and electrons arranged in three-dimensional space.
    In this intricate structure, all particles interact with each other.
    However, when a pair of atoms are stably positioned at a specific distance, we refer to their connection as a covalent bond.
    These bonds with distinct atomic distances can vary in nature, such as single or double bonds.
    Representing this complex three-dimensional object as a graph offers a practical and widely adopted abstraction, where atoms are nodes and covalent bonds act as edges~\cite{DBLP:journals/corr/DuvenaudMAGHAA15}.

    Social networks provide another domain where graphs find utility.
    They serve as valuable tools for examining patterns within the collective behavior of people, institutions, and organizations.
    By representing individuals as nodes and their relationships as edges, we can construct a graph that effectively captures groups of people and their interconnectedness.

    \section{Graph Neural Networks}
    \label{sh:graph_neural_networks}%

    Graph neural networks (GNNs) are deep learning techniques that operate on graph-structured data.
    Thanks to their impressive performance, GNNs have recently gained significant popularity as a widely adopted method for graph analysis.

    Graph Neural Networks (GNNs) are designed to process graph data and consist of multiple interconnected layers.
    Each GNN layer typically encompasses three main stages: feature extraction, aggregation, and updating, with an optional sampling stage.
    The feature extraction is employed to compress vertex features using processing functions like MLPs (Multi-Layer Perceptrons) and, together with the update stage, resembles traditional neural network inference, involving regular computational operations and memory access patterns.
    The aggregation stage traverses the graph topology to combine features from a vertex's neighboring vertices and can be accomplished using functions like sum, mean, or max.
    The update stage is utilized to apply non-linear transformations, including activation functions, GRU (Gated Recurrent Units), and MLPs, to the graph data.
    Finally, the optional sampling stage constructs a new graph through user-defined sampling operations.
    Performing these stages on large and sparse graphs can introduce dynamic computational data flow and numerous irregular memory access patterns.

    Graph Neural Networks are a group of neural networks which are designed to solve different tasks.
    Prediction tasks on graphs can generally be classified into three categories: graph-level, node-level, and edge-level predictions~\cite{sanchez-lengeling2021a}.

    In a graph-level task, the objective is to predict the property or characteristic of an entire graph.
    For instance, when considering a molecule represented as a graph, we might aim to predict attributes such as its likelihood of binding to a receptor associated with a specific disease.
    This assignment is comparable to image classification tasks, where the objective is to assign a label to an entire image.
    Similarly, in text analysis, sentiment analysis serves as a similar problem where the goal is to determine a complete sentence's overall mood or emotion in one go.

    Node-level tasks involve predicting the identity or function of individual nodes within a graph.
    One example of a node-level task is node classification in a social network.
    Given a social network graph where nodes represent individuals and edges represent relationships between them, the task is to predict the demographic attributes or characteristics (e.g., age, gender, occupation) of each node based on their connection patterns and features.
    Drawing an analogy to image processing, node-level prediction problems can be compared to image segmentation tasks, where the objective is to assign labels to each pixel in an image based on its role.
    Similarly, in text analysis, a comparable task would involve predicting the parts of speech for each word in a sentence, such as identifying whether a word is a noun, verb, adverb, and so on.


    The remaining prediction task in graphs pertains to edge prediction.
    One example of an edge-level task is link prediction in a social network.
    Given a graph representing a social network where, as before, in node-level tasks, nodes correspond to individuals and edges represent relationships between them, the edge-level task aims to predict missing or potential connections between nodes.
    This can involve predicting the likelihood of a future friendship or the probability of a collaboration between individuals based on their shared characteristics or mutual connections in the network.

    Different popular Graph Neural Network architectures have been proposed recently, some of which are more suitable for some tasks than others.
    A summary of two types of GNNs used in the experimental phase is provided in the following sections.

    \subsection{Graph Convolutional Network}
    \label{subsec:graph_convolutional_network}%

    A graph convolutional network (GCN)~\cite{DBLP:journals/corr/KipfW16, daigavane2021understanding} is a type of neural network architecture explicitly designed to operate on graph-structured data.
    GCNs aim to learn node representations by aggregating and combining information from neighboring nodes in the graph.
    The core idea behind GCNs is to perform convolution-like operations on the graph, where the convolutional filters are defined based on the graph's adjacency matrix or other graph-specific structures.
    This enables GCNs to capture and leverage the structural information encoded in the graph to make predictions or perform downstream tasks.
    GCNs have demonstrated effectiveness in various applications, including node classification, link prediction, and graph classification.

    Given an undirected graph $\mathcal{G} = (V, E)$, where $V$ represents the set of nodes (vertices), and $E$ represents the set of edges, with an adjacency matrix $\tilde{A}=A+I_N$, where $I_N$ is the identity matrix, the layer-wise propagation rule in a GCN can be expressed as:
    \begin{equation}
        \label{eq:gcn_convolution}
            H^{(l+1)} = f(\tilde{D}^{-\tfrac{1}{2}}  \tilde{A}  \tilde{D}^{-\tfrac{1}{2}}  H^{(l)}  W^{(l)})
    \end{equation}

    Where $H^{(l)} \in \mathbb{R}^{N \times D}$ is the input node features matrix, $W^{(l)}$ is a layer-specific learnable weight matrix, $\tilde{D}$ is the degree matrix defined as $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$, and $f(\cdot)$ represents a non-linear activation function applied element-wise, such as $ReLU(\cdot) = max(0, \cdot)$.
    The equation above demonstrates the propagation of node features through graph convolution, where the adjacency matrix $\tilde{A}$ captures the connectivity information of the graph, $\tilde{D}^{-\tfrac{1}{2}}$ normalizes the adjacency matrix, and $H^{(l)}  W^{(l)}$ performs a linear transformation of node features.
    The resulting $H^{(l+1)}$ represents the updated node representations after the graph convolution operation.
    In practice, multiple graph convolutional layers can be stacked to capture increasingly complex relationships and refine the node representations further.

    \subsection{Graph Isomorphism Network}
    \label{subsec:graph_isomorphism_network}%

    A Graph Isomorphism Network (GIN)~\cite{xu2019powerful, daigavane2021understanding} is a type of neural network architecture designed to operate on graph-structured data by capturing graph isomorphism, which is the property of two graphs having the same structure, inspired by the Weisfeiler-Lehman (WL) graph isomorphism test~\cite{xu2019powerful}.
    GINs aim to learn node representations that are invariant under graph isomorphism, enabling them to generalize across different graphs with similar structures.

    The learned vertex features from GIN-Conv can be directly utilized for tasks such as node classification and link prediction. It is possible to perform this model as:
    \begin{equation}
        \label{eq:gin_function}
        h_v^{(k+1)} = MLP^{(k)} \left( \left( 1 + \epsilon^{(k)} \right) \cdot h_v^{(k)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k)} \right)
    \end{equation}

    Where $h_v^{(k)}$ represents the initial node representation of node $v$, $\mathcal{N}(v)$ represents the neighborhood of node $v$, $\epsilon$ is a learnable
    parameter or a fixed scalar, $MLP( \cdot )$ represents a Multi Layer Perceptron and $h_v^{(k+1)}$ represents the updated node representations.

    In the neighborhood aggregation process of GINs, each node's representation is updated by considering its own representation and its neighbors' representations.
    The neighborhood aggregation is performed through the MLP operation, followed by non-linear activation.

    GINs are trained using graph-level objectives, such as graph classification or property prediction, and aim to learn invariant representations under graph isomorphism, allowing them to generalize well to unseen graphs with similar structures.
    However, even if the node embeddings acquired through GIN can be directly applied to tasks such as node classification and link prediction, in the case of graph classification tasks, it is necessary to use a Readout function that takes individual node embeddings as input and produces the embedding representation for the entire graph.

    The Readout function is then utilized to generate the overall representation of the graph, leveraging the individual vertex representations.
    By concatenating the results from all iterations of GINConv, the final graph representation is obtained as:
    \begin{equation}
        \label{eq:gin_readout}
        h_G = CONCAT \left( READOUT \left( \left\{ h_v^{(k)} | v \in G \right\} \right) | k = 0, 1, ..., K \right)
    \end{equation}

    Where $READOUT$ in~\ref{eq:gin_function} can be replaced with a sum operator in order to generalize the WL test~\cite{xu2019powerful}.

    \section{SODA Toolchain}


% ##########################################################################
% CHAPTER THREE - RELATED WORK
% ##########################################################################


    \chapter{Related Work}
    \label{ch:chapter_three}%

    Analysis of related works, explaining how other GNN accelerators have been built, and some
    limitations of their approach.

% ##########################################################################
% CHAPTER FOUR - PROBLEM FORMULATION
% ##########################################################################


    \chapter{Problem Formulation}
    \label{ch:chapter_four}%

    Problem formulated in a clear way, what we did and how, with open issues and thesis goals.

% ##########################################################################
% CHAPTER FIVE - TOOLCHAIN
% ##########################################################################


    \chapter{FPGA Toolchain for Graph Neural Network Acceleration}
    \label{ch:chapter_five}%

    Introduction of the way I faced the problem, with the motivation for the followed approach.
    Explanation of the toolchain in a clear way.

% ##########################################################################
% CHAPTER SIX - EXPERIMENTAL RESULTS
% ##########################################################################


    \chapter{Experimental Results}
    \label{ch:chapter_six}%

    Chapter dedicated to the outcome of the results, what I have obtained and what limitations have been encountered.
    Explaining the still open issues and research suggestions.



% ##########################################################################
% CHAPTER SEVEN - CONCLUSION
% ##########################################################################

    \chapter{Conclusions and Future Developments}
    \label{ch:conclusions}%
    Final chapter containing the main conclusions of my research
    and possible future developments.

%##########################################################################
%	BIBLIOGRAPHY
%##########################################################################

    \addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
    \bibliography{Thesis_bibliography} % The references information are stored in the file named "Thesis_bibliography.bib"

%-------------------------------------------------------------------------
%	APPENDICES
%-------------------------------------------------------------------------

    \cleardoublepage
    \addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
    \appendix

%
%    \chapter{Appendix A}
%    If you need to include an appendix to support the research in your thesis, you can place it at the end of the manuscript.
%    An appendix contains supplementary material (figures, tables, data, codes, mathematical proofs, surveys, \dots)
%    which supplement the main results contained in the previous chapters.
%
%
%    \chapter{Appendix B}
%    It may be necessary to include another appendix to better organize the presentation of supplementary material.

% LIST OF FIGURES
    \listoffigures

% LIST OF TABLES
    \listoftables

% LIST OF SYMBOLS
% Write out the List of Symbols in this page
    \chapter*{List of Symbols} % You have to include a chapter for your list of symbols (
    \begin{table}[H]
        \centering
        \begin{tabular}{lll}
            \textbf{Variable} & \textbf{Description} & \textbf{SI unit} \\\hline\\[-9px]
            $\bm{u}$        & solid displacement   & m \\[2px]
            $\bm{u}_f$        & fluid displacement   & m \\[2px]
        \end{tabular}
    \end{table}

% ACKNOWLEDGEMENTS
    \chapter*{Acknowledgements}
    Acknowledgements here...

    \cleardoublepage

\end{document}
