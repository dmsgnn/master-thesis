This chapter aims to formulate the problem rigorously, explaining the thesis's objective, motivation, and research questions that guided this work.

\section{Graph Neural Network acceleration}
\label{sec:gnn_acceleration}%

Graph neural network acceleration refers to designing and implementing hardware accelerators and co-processors to speed up the training or inference of GNNs.
A GNN accelerator aims at optimizing the execution of GNN computations, which involve iterative message-passing between nodes in a graph to update their representations based on neighboring nodes' features.
Hardware acceleration aims to improve the performance, efficiency, and capabilities of computing systems by offloading specific tasks or computations to specialized hardware components.

In particular, this thesis focuses on improving GNNs inference time, by designing specialized hardware to efficiently perform the computation-intensive operations involved in GNNs, such as matrix multiplications, aggregations, and non-linear activation functions.

A small example explains the potential impact of this thesis's objective.
Let us consider a recommendation system, in which the goal is to predict what items a user might be interested in based on their past interactions and preferences.
An example is Netflix suggesting what to watch next based on previously watched movies and ratings.
This problem can be represented as a graph, where users and movies (items) are nodes and interactions between users and movies are edges.
A GNN is an optimal choice for modeling recommendation systems, as it can effectively capture relationships and interactions between users and items.
However, as the number of users and items increases, the computational complexity of GNNs can become a significant bottleneck.
One way to address the bottleneck issue is by using a specialized GNN accelerator, which is purpose-built to handle GNN computations on extensive graphs efficiently.
By exploiting parallelism and data locality in GNN operations, it enables faster and more energy-efficient processing of the graph.

Utilizing the GNN accelerator, the recommendation system can offer real-time recommendations to users, even on edge devices with limited computational capabilities.
Ultimately, the GNN accelerator enhances the efficiency and scalability of the recommendation system, resulting in quick and precise recommendations to users. It also reduces computational and memory overhead.

\section{Motivation and objective}
\label{sec:motivation}%

As mentioned in Chapter~\ref{ch:chapter_three}, different state-of-the-art accelerators exist and use different approaches to improve GNNs' performance.
Even if various alternatives are available, almost all GNN acceleration research has implicitly focused on either developing highly efficient schemes tailored for specific GNN models or aiming for generality and flexibility to accommodate various types of GNNs with less efficiency.

The primary challenge driving the research in this thesis lies in creating a framework that optimizes performance and efficiency while retaining the necessary flexibility to adapt to diverse graph sizes, characteristics, and GNN algorithms.

The main research questions that led to this thesis can be summarised as follow:
\begin{enumerate}
    \item How is it possible to design hardware accelerators that exploit the unique characteristics of GNN computation?
    \item How is it possible to synthesize accelerators, starting from high-level programming languages, without being a hardware design expert?
    \item What are the bottlenecks of GNNs that can slow down their inference time?
    \item What are the most effective low-level optimizations for GNNs that can improve their efficiency without sacrificing model accuracy?
    %\item How does the proposed GNN hardware accelerator compare to existing software solutions?
    \item Can an automated design be generalized to different GNN models and datasets while ensuring high performance?
\end{enumerate}


Consequently, the main objectives of the thesis are:
\begin{enumerate}
    \item Investigate existing GNN models and identify bottlenecks that hinder their inference efficiency.
    \item Develop an FPGA toolchain for GNN acceleration, allowing seamless integration with various GNN models and datasets.
    \item Explore low-level optimizations for GNNs to improve hardware performance.
    \item Synthesize hardware accelerators tailored to GNNs, leveraging parallelism and memory optimizations to accelerate graph computations.
    \item Implement and evaluate the proposed GNN accelerators on FPGA\@.
\end{enumerate}


