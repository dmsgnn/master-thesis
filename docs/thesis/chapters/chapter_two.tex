This chapter provides essential background information to enhance understanding of the thesis content and objectives.
It begins by introducing the graph data structure, which is crucial for comprehending Graph Neural Networks.
Additionally, the chapter provides an introduction to Graph Neural Networks, outlining their capabilities and exploring various applications.
Furthermore, it introduces two essential tools, SODA and Bambu, which are integral parts of the SODA Toolchain that served as the foundation for this research study.

\section{Graphs}
\label{sh:graphs}%

\textit{Graphs} are a data structure representing a collection of objects, known as vertices or nodes, and a set of edges~\cite{DBLP:journals/corr/abs-1812-08434}.
In a graph, the edges can be either directed or undirected, as shown in Figure~\ref{fig:directed_vs_undirected}, and they typically connect two vertices, which may or may not be distinct.
The vertices represent entities or elements, and the edges represent their relationships or connections.

\begin{figure}[b]
    \centering
    \subfloat[Directed Graph\label{fig:directed_graph}]{
        \includegraphics[width=0.2\textwidth]{Images/directed_graph}
    }
    %\quad
    \hspace{0.15\textwidth}
    \subfloat[Undirected Graph\label{fig:undirected_graph}]{
        \captionsetup{width=.4\textwidth}
        \includegraphics[width=0.2\textwidth]{Images/undirected_graph}
    }
    %\caption[Shorter caption]{This is a very long caption you don't want to appear in the List of Figures.}
    \caption{Example of directed and undirected graphs}
    \label{fig:directed_vs_undirected}
\end{figure}

Graphs serve as a versatile tool for describing diverse forms of data.
Molecules, the fundamental units of matter, are composed of atoms and electrons arranged in three-dimensional space.
In this intricate structure, all particles interact with each other.
However, when a pair of atoms are stably positioned at a specific distance, we refer to their connection as a covalent bond.
These bonds with distinct atomic distances can vary in nature, such as single or double bonds.
Representing this complex three-dimensional object as a graph offers a practical and widely adopted abstraction, where atoms are nodes and covalent bonds act as edges~\cite{DBLP:journals/corr/DuvenaudMAGHAA15}.

Social networks provide another domain where graphs find utility.
They serve as valuable tools for examining patterns within the collective behavior of people, institutions, and organizations.
By representing individuals as nodes and their relationships as edges, we can construct a graph that effectively captures groups of people and their interconnectedness.


\section{Graph Neural Networks}
\label{sh:graph_neural_networks}%

Graph neural networks (GNNs) are deep learning techniques that operate on graph-structured data.
Thanks to their impressive performance, GNNs have recently gained significant popularity as a widely adopted method for graph analysis.

Graph Neural Networks (GNNs) are designed to process graph data and consist of multiple interconnected layers.
Each GNN layer typically encompasses three main stages: feature extraction, aggregation, and updating, with an optional sampling stage.
The feature extraction is employed to compress vertex features using processing functions like MLPs (Multi-Layer Perceptrons) and, together with the update stage, resembles traditional neural network inference, involving regular computational operations and memory access patterns.
The aggregation stage traverses the graph topology to combine features from a vertex's neighboring vertices and can be accomplished using functions like sum, mean, or max.
The update stage is utilized to apply non-linear transformations, including activation functions, GRU (Gated Recurrent Units), and MLPs, to the graph data.
Finally, the optional sampling stage constructs a new graph through user-defined sampling operations.
Performing these stages on large and sparse graphs can introduce dynamic computational data flow and numerous irregular memory access patterns.

Graph Neural Networks are a group of neural networks which are designed to solve different tasks.
Prediction tasks on graphs can generally be classified into three categories: graph-level, node-level, and edge-level predictions~\cite{sanchez-lengeling2021a}.

In a graph-level task, the objective is to predict the property or characteristic of an entire graph.
For instance, when considering a molecule represented as a graph, we might aim to predict attributes such as its likelihood of binding to a receptor associated with a specific disease.
This assignment is comparable to image classification tasks, where the objective is to assign a label to an entire image.
Similarly, in text analysis, sentiment analysis serves as a similar problem where the goal is to determine a complete sentence's overall mood or emotion in one go.

Node-level tasks involve predicting the identity or function of individual nodes within a graph.
One example of a node-level task is node classification in a social network.
Given a social network graph where nodes represent individuals and edges represent relationships between them, the task is to predict the demographic attributes or characteristics (e.g., age, gender, occupation) of each node based on their connection patterns and features.
Drawing an analogy to image processing, node-level prediction problems can be compared to image segmentation tasks, where the objective is to assign labels to each pixel in an image based on its role.
Similarly, in text analysis, a comparable task would involve predicting the parts of speech for each word in a sentence, such as identifying whether a word is a noun, verb, adverb, and so on.


The remaining prediction task in graphs pertains to edge prediction.
One example of an edge-level task is link prediction in a social network.
Given a graph representing a social network where, as before, in node-level tasks, nodes correspond to individuals and edges represent relationships between them, the edge-level task aims to predict missing or potential connections between nodes.
This can involve predicting the likelihood of a future friendship or the probability of a collaboration between individuals based on their shared characteristics or mutual connections in the network.

Different popular Graph Neural Network architectures have been proposed recently, some of which are more suitable for some tasks than others.
A summary of two types of GNNs used in the experimental phase is provided in the following sections.

\subsection{Graph Convolutional Network}
\label{subsec:graph_convolutional_network}%

A graph convolutional network (GCN)~\cite{DBLP:journals/corr/KipfW16, daigavane2021understanding} is a type of neural network architecture explicitly designed to operate on graph-structured data.
GCNs aim to learn node representations by aggregating and combining information from neighboring nodes in the graph.
The core idea behind GCNs is to perform convolution-like operations on the graph, where the convolutional filters are defined based on the graph's adjacency matrix or other graph-specific structures.
This enables GCNs to capture and leverage the structural information encoded in the graph to make predictions or perform downstream tasks.
GCNs have demonstrated effectiveness in various applications, including node classification, link prediction, and graph classification.

Given an undirected graph $\mathcal{G} = (V, E)$, where $V$ represents the set of nodes (vertices), and $E$ represents the set of edges, with an adjacency matrix $\tilde{A}=A+I_N$, where $I_N$ is the identity matrix, the layer-wise propagation rule in a GCN can be expressed as:
\begin{equation}
    \label{eq:gcn_convolution}
    H^{(l+1)} = f(\tilde{D}^{-\tfrac{1}{2}}  \tilde{A}  \tilde{D}^{-\tfrac{1}{2}}  H^{(l)}  W^{(l)})
\end{equation}

Where $H^{(l)} \in \mathbb{R}^{N \times D}$ is the input node features matrix, $W^{(l)}$ is a layer-specific learnable weight matrix, $\tilde{D}$ is the degree matrix defined as $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$, and $f(\cdot)$ represents a non-linear activation function applied element-wise, such as $ReLU(\cdot) = max(0, \cdot)$.
The equation above demonstrates the propagation of node features through graph convolution, where the adjacency matrix $\tilde{A}$ captures the connectivity information of the graph, $\tilde{D}^{-\tfrac{1}{2}}$ normalizes the adjacency matrix, and $H^{(l)}  W^{(l)}$ performs a linear transformation of node features.
The resulting $H^{(l+1)}$ represents the updated node representations after the graph convolution operation.
In practice, multiple graph convolutional layers can be stacked to capture increasingly complex relationships and refine the node representations further.

\subsection{Graph Isomorphism Network}
\label{subsec:graph_isomorphism_network}%

A Graph Isomorphism Network (GIN)~\cite{xu2019powerful, daigavane2021understanding} is a type of neural network architecture designed to operate on graph-structured data by capturing graph isomorphism, which is the property of two graphs having the same structure, inspired by the Weisfeiler-Lehman (WL) graph isomorphism test~\cite{xu2019powerful}.
GINs aim to learn node representations that are invariant under graph isomorphism, enabling them to generalize across different graphs with similar structures.

The learned vertex features from GIN-Conv can be directly utilized for tasks such as node classification and link prediction. It is possible to perform this model as:
\begin{equation}
    \label{eq:gin_function}
    h_v^{(k+1)} = MLP^{(k)} \left( \left( 1 + \epsilon^{(k)} \right) \cdot h_v^{(k)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k)} \right)
\end{equation}

Where $h_v^{(k)}$ represents the initial node representation of node $v$, $\mathcal{N}(v)$ represents the neighborhood of node $v$, $\epsilon$ is a learnable
parameter or a fixed scalar, $MLP( \cdot )$ represents a Multi Layer Perceptron and $h_v^{(k+1)}$ represents the updated node representations.

In the neighborhood aggregation process of GINs, each node's representation is updated by considering its own representation and its neighbors' representations.
The neighborhood aggregation is performed through the MLP operation, followed by non-linear activation.

GINs are trained using graph-level objectives, such as graph classification or property prediction, and aim to learn invariant representations under graph isomorphism, allowing them to generalize well to unseen graphs with similar structures.
However, even if the node embeddings acquired through GIN can be directly applied to tasks such as node classification and link prediction, in the case of graph classification tasks, it is necessary to use a Readout function that takes individual node embeddings as input and produces the embedding representation for the entire graph.

The Readout function is then utilized to generate the overall representation of the graph, leveraging the individual vertex representations.
By concatenating the results from all iterations of GINConv, the final graph representation is obtained as:
\begin{equation}
    \label{eq:gin_readout}
    h_G = CONCAT \left( READOUT \left( \left\{ h_v^{(k)} | v \in G \right\} \right) | k = 0, 1, ..., K \right)
\end{equation}

Where $READOUT$ in~\ref{eq:gin_function} can be replaced with a sum operator in order to generalize the WL test~\cite{xu2019powerful}.


\section{SODA Toolchain}
\label{sh:soda}%

SODA~\cite{9786533} is a software-defined accelerator synthesizer.
It enables the creation of highly specialized accelerators from algorithms designed in high-level programming frameworks.
The synthesizer comprises a compiler-based frontend that interfaces with high-level programming frameworks, applying advanced optimizations.
It also includes a compiler-based backend responsible for generating Verilog code and interfacing with external tools to compile the final design, which can be applied to application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs).

SODA's exceptional power lies in its ability to offer a fully automated end-to-end hardware compiler, eliminating the need for human intervention and any modifications to the input code.
The SODA synthesizer framework comprises two main components: a compiler-based frontend and a compiler-based hardware generation engine.
This framework seamlessly integrates with high-level Python frameworks by accepting their input descriptions, which are then translated by the frontend into a high-level intermediate representation (IR).
Leveraging the multi-level intermediate representation (MLIR), the frontend facilitates hardware/software partitioning of algorithm specifications and performs architecture-independent optimizations.
Following this, it generates a low-level IR (LLVM IR) that is utilized by the hardware generation engine, PandA-Bambu~\cite{9586110}.
PandA-Bambu can accept LLVM IR as input, making it a cutting-edge open-source HLS tool.
Throughout the entire SODA toolchain, compiler passes are employed to implement optimizations at all levels, greatly influencing the generated hardware designs' performance, area, and power characteristics.

\subsection{SODA-OPT Frontend}
\label{subsec:soda_frontend}%

SODA-OPT, the high-level compiler frontend of the SODA synthesizer, performs search, outlining, optimization, dispatching, and acceleration pass on the input program.
Its primary objective is to prepare the program for hardware synthesis, targeting either FPGAs or ASICs.
To accomplish these tasks, SODA-OPT relies on and extends the MLIR framework.
MLIR is a framework that facilitates the development of reusable, extensible, and modular compiler infrastructure by defining dialects.
These dialects serve as self-contained intermediate representations (IRs) that adhere to the meta-IR syntax of MLIR.
By utilizing dialects, code can be modeled at different levels of abstraction, allowing for specialized representations that aid in specific compiler optimizations.

 Code regions selected for hardware acceleration undergo an optimization pipeline that progressively lowers them through various MLIR dialects until they are ultimately translated into an LLVM IR format tailored explicitly for hardware synthesis.
On the other hand, the host module is lowered into an LLVM IR file containing runtime calls to control the generated custom accelerators.

\subsection{SODA Synthesizer Frontend}
\label{subsec:soda_backend}%

Bambu, the SODA synthesizer backend, harnesses cutting-edge HLS techniques to produce accelerator designs using the low-level LLVM IR generated by the SODA frontend.
Bambu boasts multiple frontends based on standard compilers such as GCC or CLANG.
It constructs an internal IR to execute HLS steps and generates designs in HDL formats, such as Verilog or VHDL.
In addition to synthesizable HDL, Bambu can automatically generate testbenches for verification purposes.
Using Bambu, the SODA synthesizer can target both FPGAs and ASICs.

Bambu is optimized to handle a broad range of C and C++ constructs while also being able to process LLVM IR through its internal Clang frontend.
Through SODA-OPT, Bambu can be connected with MLIR code.
The LLVM IR generated after SODA-OPT's high-level optimizations undergoes explicit restructuring for HLS, resulting in more efficient accelerators than direct translation from MLIR to LLVM IR.

Bambu generates designs at the Register Transfer Level (RTL), adhering to the Finite State Machine with Datapath (FSMD) model.
These generated accelerators can subsequently be integrated into larger system-level designs, with or without microcontrollers controlling the execution.
